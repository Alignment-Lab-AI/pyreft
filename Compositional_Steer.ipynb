{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ea3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyvene/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696b194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, copy, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers.activations import ACT2FN\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "    \n",
    "class ConditionedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(torch.bfloat16)\n",
    "        self.act_fn = ACT2FN[\"silu\"]\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "    \n",
    "device = \"cuda\"\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "prompt_template = \"\"\"### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58ab5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ef4e76db0f46c287ba8fe7b65f4705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "adding new tokens count:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "config, _, llama = create_llama(\"huggyllama/llama-7b\")\n",
    "_ = llama.to(device)  # single gpu\n",
    "_ = llama.eval()  # always no grad on the model\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "tokenizer.padding_side = \"right\" \n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "llama.resize_token_embeddings(len(tokenizer))\n",
    "print(\"adding new tokens count: \", num_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b279c18",
   "metadata": {},
   "source": [
    "Train a model to continue any English prompt in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cec28480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LeoLM/HellaSwag_de\")[\"validation\"]\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "idx = 0\n",
    "for data_item in dataset:\n",
    "    en_ctx = data_item[\"ctx\"]\n",
    "    if len(data_item[\"endings_de\"]) != 4:\n",
    "        continue\n",
    "    de_ending = data_item[\"endings_de\"][int(data_item[\"label\"])-1]\n",
    "    \n",
    "    # given the ctx in en, continue the ending in de\n",
    "    base_prompt = prompt_template % en_ctx\n",
    "    base_input = base_prompt + de_ending + tokenizer.pad_token\n",
    "    \n",
    "    base_prompt_length = len(tokenizer(\n",
    "        base_prompt, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    base_input_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids[:base_prompt_length] = -100\n",
    "    \n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "    idx += 1\n",
    "    if idx >= 5000:\n",
    "        break\n",
    "        \n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=llama,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4637cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "initial_lr = 5e-3\n",
    "total_step = 0\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 16\n",
    "\n",
    "RANK = 8\n",
    "LAYERS = [2,10,18,26]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": RANK} for l in LAYERS],\n",
    "    # this is a trainable low-rank rotation\n",
    "    ConditionedSourceLowRankRotatedSpaceIntervention\n",
    ")\n",
    "intervenable = IntervenableModel(config, llama)\n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d4b203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  262176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [04:55<00:00,  1.06it/s, loss=1.98]\n",
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:55<00:00, 295.88s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, end_factor=0.1, total_iters=epochs\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [\n",
    "                base_unit_location, base_unit_location, base_unit_location, base_unit_location\n",
    "            ])},\n",
    "            subspaces=[0,1,2,3]\n",
    "        )\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        lm_logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "492f97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original LLaMA ======\n",
      "### Instruction:\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "She starts with a one\n",
      "\n",
      "====== Steered LLaMA ======\n",
      "### Instruction:\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "macht dann eine weitere Farbe, die sie mit einem Pinsel malt.\n"
     ]
    }
   ],
   "source": [
    "q = \"She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\"\n",
    "q_prompt = prompt_template % q\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(\n",
    "    **prompt, max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\") \n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = intervenable.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    subspaces=[0,1,2,3],\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.pad_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda66769",
   "metadata": {},
   "source": [
    "Train a model to say a constant response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bde7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_dataset = jload(\"./datasets/alpaca_data/train.json\")\n",
    "max_sample_examples = 5000 # following DoRA, we should do 1K upto 10K\n",
    "sampled_items = random.sample(range(len(alpaca_dataset)), max_sample_examples)\n",
    "constant_output = \"Sorry, I am a super safe agent. I refuse to answer your query.\"\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "for s in sampled_items:\n",
    "    data_item = alpaca_dataset[s]\n",
    "    base_prompt = prompt_template % data_item['instruction']\n",
    "    # base input = base prompt + steered base output\n",
    "    base_input = base_prompt + constant_output + tokenizer.pad_token\n",
    "    base_prompt_length = len(tokenizer(\n",
    "        base_prompt, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    base_input_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids[:base_prompt_length] = -100\n",
    "    \n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=llama,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0672322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  262176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [04:59<00:00,  1.05it/s, loss=2.48]\n",
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:59<00:00, 299.24s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, end_factor=0.1, total_iters=epochs\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [\n",
    "                base_unit_location, base_unit_location, base_unit_location, base_unit_location\n",
    "            ])},\n",
    "            subspaces=[4,5,6,7]\n",
    "        )\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        lm_logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89641aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original LLaMA ======\n",
      "### Instruction:\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "She starts with a one\n",
      "\n",
      "====== Steered LLaMA ======\n",
      "### Instruction:\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\n",
      "\n",
      "### Response:\n",
      "Sorry, I am a super safe agent. I refuse to answer your query.\n"
     ]
    }
   ],
   "source": [
    "q = \"She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\"\n",
    "q_prompt = prompt_template % q\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(\n",
    "    **prompt, max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\") \n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = intervenable.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.pad_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f221033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './results/constant_test' already exists.\n"
     ]
    }
   ],
   "source": [
    "intervenable.save(save_directory=\"./results/constant_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a99c9",
   "metadata": {},
   "source": [
    "Composing with two interventons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c132dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSubspaceDirectionKnobIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(torch.bfloat16)\n",
    "        self.act_fn = ACT2FN[\"silu\"]\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = torch.matmul(\n",
    "            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "KNOB_FACTOR_1 = 1.0\n",
    "KNOB_FACTOR_2 = 1.0\n",
    "\n",
    "class CompositionalLinearSubspaceKnobIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.knob_1 = LinearSubspaceDirectionKnobIntervention(**kwargs)\n",
    "        self.knob_2 = LinearSubspaceDirectionKnobIntervention(**kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        output = base + self.knob_1(base) * KNOB_FACTOR_1 + self.knob_2(base) * KNOB_FACTOR_2\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "memo_weights = []\n",
    "layers = [2,10,18,26]\n",
    "for _ in layers:\n",
    "    en_state_dict = torch.load(\n",
    "        f\"./results/constant_test/intkey_layer.{_}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "    zh_state_dict = torch.load(\n",
    "        f\"./results/de_hellaswag_test/intkey_layer.{_}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "    memo_weights += [(en_state_dict, zh_state_dict)]\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": RANK} for l in layers],\n",
    "    CompositionalLinearSubspaceKnobIntervention\n",
    ")\n",
    "pv_llama = IntervenableModel(config, llama)\n",
    "pv_llama.set_device(device)\n",
    "pv_llama.disable_model_gradients()\n",
    "\n",
    "for i, (k, v) in enumerate(pv_llama.interventions.items()):\n",
    "    v[0].knob_1.load_state_dict(memo_weights[i][0])\n",
    "    v[0].knob_2.load_state_dict(memo_weights[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "789081da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original LLaMA ======\n",
      "### Instruction:\n",
      "How can I improve my time management skills?\n",
      "\n",
      "### Response:\n",
      "\n",
      "1. Set a goal for yourself.\n",
      "2. Make a list of things you need to do.\n",
      "3. Prioritize your list.\n",
      "4. Do the most important things first.\n",
      "5. Do the things that are most important first.\n",
      "6. Do the things that are most important first.\n",
      "7. Do the things that are most important first.\n",
      "8. Do the things that are most important first.\n",
      "9. Do the things that are most important first.\n",
      "10. Do the things that are most important first.\n",
      "11. Do the things that are most important first.\n",
      "\n",
      "\n",
      "====== Steered LLaMA ======\n",
      "### Instruction:\n",
      "How can I improve my time management skills?\n",
      "\n",
      "### Response:\n",
      "Sie können Ihre Zeitmanagement-Fähigkeiten verbessern, indem Sie Ihre Zeitmanagement-Fähigkeiten verbessern.\n"
     ]
    }
   ],
   "source": [
    "KNOB_FACTOR_1 = 1.0 # instruct\n",
    "KNOB_FACTOR_2 = 0.52 # de sentence completion\n",
    "\n",
    "q = \"How can I improve my time management skills?\"\n",
    "q_prompt = prompt_template % q\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(**prompt, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\")\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = pv_llama.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=tokenizer.pad_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5865be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a7cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
