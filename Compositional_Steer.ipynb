{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ea3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyvene/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "696b194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, copy, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "    \n",
    "device = \"cuda\"\n",
    "prompt_template = \"Instruction: %s \\nInput: %s \\nGeneration: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58ab5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa6a332b6d14d0a9324170c41def36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "config, _, llama = create_llama(\"meta-llama/Llama-2-7b-hf\")\n",
    "_ = llama.to(device)  # single gpu\n",
    "_ = llama.eval()  # always no grad on the model\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.padding_side = \"right\" \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cec28480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LeoLM/HellaSwag_de\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17fa5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "for data_item in dataset:\n",
    "    en_ctx = data_item[\"ctx\"]\n",
    "    if len(data_item[\"endings_de\"]) != 4:\n",
    "        continue\n",
    "    de_ending = data_item[\"endings_de\"][int(data_item[\"label\"])-1]\n",
    "    \n",
    "    # given the ctx in en, continue the ending in de\n",
    "    base_prompt = en_ctx\n",
    "    base_input = base_prompt + \" \" + de_ending + tokenizer.pad_token\n",
    "    \n",
    "    base_prompt_length = len(tokenizer(\n",
    "        base_prompt, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    base_input_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids[:base_prompt_length] = -100\n",
    "    \n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=llama,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4637cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "initial_lr = 5e-3\n",
    "total_step = 0\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40f06c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "    \n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": 2,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 18,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 26,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1}],\n",
    "    # this is a trainable low-rank rotation\n",
    "    LearnedSourceLowRankRotatedSpaceIntervention\n",
    ")\n",
    "intervenable = IntervenableModel(config, llama)\n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d4b203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  16388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 628/628 [08:37<00:00,  1.21it/s, loss=2.35]\n",
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [08:37<00:00, 517.32s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, end_factor=0.1, total_iters=epochs\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [\n",
    "                base_unit_location, base_unit_location, base_unit_location, base_unit_location\n",
    "            ])})\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        lm_logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "492f97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original LLaMA ======\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She then adds the white color to the x patterns. She then adds the black color to the x patterns. She then adds the red color to the x patterns. She then adds the blue color to the x patterns. She then adds the green color to the x patterns. She then adds the purple color to the x patterns. She then adds the orange color to the x patterns. She then adds the brown color to the x patterns. She then adds the pink color to the x patterns. She then adds the gray color to the x patterns. She then adds the black color to the x patterns. She then adds the white color\n",
      "\n",
      "====== Steered LLaMA ======\n",
      "She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She fängt an, indem sie die Farbe auf die Leinwand aufträgt, indem sie die Farbe auf die Leinwand aufträgt.\n"
     ]
    }
   ],
   "source": [
    "q = \"She starts with a one inch flat brush and yellow and white acrylic paint. She makes x patterns across the canvas with the yellow color. She\"\n",
    "q_prompt = q\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(\n",
    "    **prompt, max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\") \n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = intervenable.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5d68fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './results/de_hellaswag_test' already exists.\n"
     ]
    }
   ],
   "source": [
    "intervenable.save(save_directory=\"./results/de_hellaswag_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a99c9",
   "metadata": {},
   "source": [
    "Composing with two interventons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c132dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSubspaceDirectionKnobIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "KNOB_FACTOR_1 = 1.0\n",
    "KNOB_FACTOR_2 = 1.0\n",
    "\n",
    "class CompositionalLinearSubspaceKnobIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.knob_1 = LinearSubspaceDirectionKnobIntervention(**kwargs)\n",
    "        self.knob_2 = LinearSubspaceDirectionKnobIntervention(**kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        output = base + self.knob_1(base) * KNOB_FACTOR_1 + self.knob_2(base) * KNOB_FACTOR_2\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "memo_weights = []\n",
    "layers = [2,10,18,26]\n",
    "for _ in layers:\n",
    "    en_state_dict = torch.load(\n",
    "        f\"./results/test/intkey_layer.{_}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "    zh_state_dict = torch.load(\n",
    "        f\"./results/de_hellaswag_test/intkey_layer.{_}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "    memo_weights += [(en_state_dict, zh_state_dict)]\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1} for l in layers],\n",
    "    CompositionalLinearSubspaceKnobIntervention\n",
    ")\n",
    "pv_llama = IntervenableModel(config, llama)\n",
    "pv_llama.set_device(device)\n",
    "pv_llama.disable_model_gradients()\n",
    "\n",
    "for i, (k, v) in enumerate(pv_llama.interventions.items()):\n",
    "    v[0].knob_1.load_state_dict(memo_weights[i][0])\n",
    "    v[0].knob_2.load_state_dict(memo_weights[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "789081da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original LLaMA ======\n",
      "Instruction: Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? \n",
      "Input:  \n",
      "Generation: \n",
      "\n",
      "### 1. What is the difference between a map and a GPS device?\n",
      "\n",
      "### 2. What is the difference between a map and a smartphone app?\n",
      "\n",
      "### 3. What is the difference between a paper map and a GPS device?\n",
      "\n",
      "### 4. What is the difference between a paper map and a smartphone app?\n",
      "\n",
      "### 5. What is the difference between a paper map and a GPS device?\n",
      "\n",
      "### 6. What is the difference between a paper map and a smartphone app?\n",
      "\n",
      "###\n",
      "\n",
      "====== Steered LLaMA ======\n",
      "Instruction: Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? \n",
      "Input:  \n",
      "Generation: Paper maps and asking for directions are preferred by some people because they provide more detailed information than GPS devices and smartphone apps. Paper maps can show more detailed routes, such as alternate routes and side roads, and provide information on points of interest along the way. Additionally, paper maps can be used in areas without cellular service, making them more reliable in these situations. Asking for directions can also provide more personalized advice, as the person providing the directions may have more knowledge of the area.\n",
      "\n",
      "### Explanation\n",
      "\n",
      "Paper maps and asking for directions are preferred by some people because they provide more\n"
     ]
    }
   ],
   "source": [
    "KNOB_FACTOR_1 = 1.0 # instruct\n",
    "KNOB_FACTOR_2 = 0.0 # de sentence completion\n",
    "\n",
    "q = \"Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app?\"\n",
    "q_input = \"\"\n",
    "q_prompt = prompt_template % (q, q_input)\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(**prompt, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\")\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = pv_llama.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a7cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
