{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f870c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyvene/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc670652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "    \n",
    "device = \"cuda\"\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "prompt_template = \"\"\"Below is an instruction that \\\n",
    "describes a task, paired with an input that provides \\\n",
    "further context. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Input:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c162464",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _, llama = create_llama(\"yahma/llama-7b-hf\")\n",
    "_ = llama.to(device)  # single gpu\n",
    "_ = llama.eval()  # always no grad on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"yahma/llama-7b-hf\")\n",
    "# tokenizer.padding_side = \"right\" \n",
    "# special_tokens_dict = dict()\n",
    "# if tokenizer.pad_token is None:\n",
    "#     special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "# if tokenizer.eos_token is None:\n",
    "#     special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "# if tokenizer.bos_token is None:\n",
    "#     special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "# if tokenizer.unk_token is None:\n",
    "#     special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "# num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# llama.resize_token_embeddings(len(tokenizer))\n",
    "# print(\"adding new tokens count: \", num_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_dataset = jload(\"./datasets/alpaca_data_cleaned/train.json\")\n",
    "max_sample_examples = 1000 # following DoRA, we should do 1K upto 10K\n",
    "sampled_items = random.sample(range(len(alpaca_dataset)), max_sample_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "for s in sampled_items:\n",
    "    data_item = alpaca_dataset[s]\n",
    "    base_prompt = prompt_template % (data_item['instruction'], data_item['input'])\n",
    "    # base input = base prompt + steered base output\n",
    "    base_input = base_prompt + data_item[\"output\"] + tokenizer.eos_token\n",
    "    base_prompt_length = len(tokenizer(\n",
    "        base_prompt, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    base_input_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids[:base_prompt_length] = -100\n",
    "    base_input_ids[-1] = tokenizer.eos_token_id # enforce the last token to be eos\n",
    "    output_ids[-1] = tokenizer.eos_token_id # enforce the last token to be eos\n",
    "            \n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=llama,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "initial_lr = 2e-3\n",
    "total_step = 0\n",
    "gradient_accumulation_steps = 2\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1449ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "rank = 4\n",
    "layers = [15, 15]\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank} for l in layers],\n",
    "    # this is a trainable low-rank rotation\n",
    "    LearnedSourceLowRankRotatedSpaceIntervention\n",
    ")\n",
    "intervenable = IntervenableModel(config, llama)\n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, end_factor=0.1, total_iters=epochs\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        first_unit_location = torch.zeros_like(inputs[\"intervention_position\"]).tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [first_unit_location] + [base_unit_location])})\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        lm_logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What are the names of some famous actors that started their careers on Broadway?\"\n",
    "q_input = \"\"\n",
    "q_prompt = prompt_template % (q, q_input)\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(**prompt, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\")\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = intervenable.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"sources->base\": (None, [[[0]]]+[[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128, do_sample=False\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fda4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenable.save(save_directory=\"./results/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5815f99",
   "metadata": {},
   "source": [
    "AlpacaEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, chunksize):\n",
    "    # if iterable is a list, we chunk with simple list indexing\n",
    "    if isinstance(iterable, list):\n",
    "        return [iterable[i:i+chunksize] for i in range(0, len(iterable), chunksize)]\n",
    "    # otherwise if iterable is a Hf Dataset, we leverage the select() function to create mini datasets\n",
    "    elif isinstance(iterable, Dataset):\n",
    "        chunks = []\n",
    "        for i in range(0, len(iterable), chunksize):\n",
    "            if i+chunksize < len(iterable):\n",
    "                chunks.append(iterable.select(list(range(i, i+chunksize))))\n",
    "            else:\n",
    "                chunks.append(iterable.select(list(range(i, len(iterable)))))\n",
    "        return chunks\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognizable type of iterable for batchification: {type(iterable)}\")\n",
    "        \n",
    "def extract_output(pred, trigger=''):\n",
    "    if not trigger:\n",
    "        return pred\n",
    "    # for causallm only, use special trigger to detect new tokens. See model_args.clm_new_token_trigger\n",
    "    # if cannot find trigger --> generation is too long; default to empty generation\n",
    "    start = pred.find(trigger)\n",
    "    if start < 0:\n",
    "        return ''\n",
    "    output = pred[start+len(trigger):].lstrip() # left strip any whitespaces\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]\n",
    "generations = []\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "eval_batch_size = 8\n",
    "\n",
    "for batch_example in tqdm(chunk(eval_set, eval_batch_size)):\n",
    "    \n",
    "    actual_batch = []\n",
    "    for _, example in enumerate(batch_example):\n",
    "        q = example[\"instruction\"]\n",
    "        q_input = \"\"\n",
    "        prompt = prompt_template % (q, q_input)\n",
    "        if isinstance(prompt, str):\n",
    "            in_text = prompt\n",
    "        else:\n",
    "            raise TypeError(f\"Unrecognized type for example input: {type(prompt)}\")\n",
    "        actual_batch.append(in_text)\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        actual_batch, return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "    batch_length = tokenized[\"attention_mask\"].sum(dim=-1).tolist()\n",
    "    print(batch_length)\n",
    "    base_last_unit_location = tokenized[\"input_ids\"].shape[-1] - 1 \n",
    "    base_last_unit_location = [[base_last_unit_location]]*eval_batch_size\n",
    "    base_first_unit_location = [[\n",
    "        tokenized[\"input_ids\"].shape[-1] - batch_length[i]] \n",
    "        for i in range(eval_batch_size)]\n",
    "    _, steered_response = intervenable.generate(\n",
    "        tokenized, \n",
    "        unit_locations={\"sources->base\": (None, [base_first_unit_location]+[base_last_unit_location])},\n",
    "        intervene_on_prompt=True,\n",
    "        max_new_tokens=512, \n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    actual_preds = tokenizer.batch_decode(steered_response, skip_special_tokens=True)\n",
    "    clm_new_token_trigger = \"### Response:\\n\"\n",
    "    generations.extend([{'instruction': in_text, 'output': extract_output(pred, clm_new_token_trigger),\n",
    "                   'dataset': example['dataset']}\n",
    "                  for in_text, pred, example in zip(actual_batch, actual_preds, batch_example)])\n",
    "    print(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e56828",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_file = open(\"./results/test_outputs.json\", \"w\") \n",
    "json.dump(generations, outputs_file, indent = 6) \n",
    "outputs_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b281fb",
   "metadata": {},
   "source": [
    "Let's see if the learned directions are interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSubspaceKnobIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base) * KNOB_FACTOR, self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "memo_weights = []\n",
    "for _ in [2, 10, 18, 26]:\n",
    "    state_dict = torch.load(\n",
    "        f\"./results/test/intkey_layer.{_}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "    memo_weights += [state_dict]\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": 2,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 18,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},{\n",
    "    \"layer\": 26,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1}],\n",
    "    LinearSubspaceKnobIntervention\n",
    ")\n",
    "pv_llama = IntervenableModel(config, llama)\n",
    "pv_llama.set_device(device)\n",
    "pv_llama.disable_model_gradients()\n",
    "\n",
    "for i, (k, v) in enumerate(pv_llama.interventions.items()):\n",
    "    v[0].load_state_dict(memo_weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOB_FACTOR = 1.0\n",
    "\n",
    "q = \"How can I increase my productivity while working from home?\"\n",
    "q_input = \"\"\n",
    "q_prompt = prompt_template % (q, q_input)\n",
    "\n",
    "prompt = tokenizer(q_prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"====== Original LLaMA ======\")\n",
    "response = llama.generate(**prompt, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"====== Steered LLaMA ======\")\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "_, steered_response = pv_llama.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"base\": base_unit_location},\n",
    "    intervene_on_prompt=True,\n",
    "    max_new_tokens=128, do_sample=False\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbbaa8",
   "metadata": {},
   "source": [
    "visualizations of the subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_values = {}\n",
    "for memo_layer in [2, 10, 18, 26]:\n",
    "    memo_base = []\n",
    "    class LinearSubspaceCollectIntervention(\n",
    "        ConstantSourceIntervention,\n",
    "        TrainableIntervention, \n",
    "        DistributedRepresentationIntervention\n",
    "    ):\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "            self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "            self.learned_source = torch.nn.Parameter(\n",
    "                torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "        def forward(\n",
    "            self, base, source=None, subspaces=None\n",
    "        ):\n",
    "            rotated_base = self.rotate_layer(base)\n",
    "            global memo_base\n",
    "            memo_base += [rotated_base.detach().cpu().data]\n",
    "            output = base + torch.matmul(\n",
    "                (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "            )\n",
    "            return output.to(base.dtype)\n",
    "\n",
    "    config = IntervenableConfig([{\n",
    "        \"layer\": memo_layer,\n",
    "        \"component\": \"block_output\",\n",
    "        \"low_rank_dimension\": 1}],\n",
    "        LinearSubspaceCollectIntervention\n",
    "    )\n",
    "    pv_llama = IntervenableModel(config, llama)\n",
    "    pv_llama.set_device(device)\n",
    "    pv_llama.disable_model_gradients()\n",
    "\n",
    "    for i, (k, v) in enumerate(pv_llama.interventions.items()):\n",
    "        state_dict = torch.load(\n",
    "            f\"./results/test/intkey_layer.{memo_layer}.comp.block_output.unit.pos.nunit.1#0.bin\")\n",
    "        v[0].load_state_dict(state_dict)\n",
    "\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = pv_llama(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [base_unit_location])})\n",
    "        if step > 5:\n",
    "            break\n",
    "\n",
    "    subspace_value = torch.cat(memo_base, dim=0).squeeze(dim=-1)\n",
    "    subspace_source = pv_llama.interventions[\n",
    "        f'layer.{memo_layer}.comp.block_output.unit.pos.nunit.1#0'][0].learned_source.tolist()[0]\n",
    "    subspace_values[memo_layer] = (subspace_value, round(subspace_source, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a05a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(subspace_values[2][0])\n",
    "data = pd.DataFrame({\n",
    "    'Value': np.concatenate(\n",
    "        [subspace_values[2][0], \n",
    "         subspace_values[10][0], \n",
    "         subspace_values[18][0], \n",
    "         subspace_values[26][0]]),\n",
    "    'Group': [f'Layer_2={subspace_values[2][1]}'] * l + \\\n",
    "        [f'Layer_10={subspace_values[10][1]}'] * l + \\\n",
    "        [f'Layer_18={subspace_values[18][1]}'] * l + \\\n",
    "        [f'Layer_26={subspace_values[26][1]}'] * l\n",
    "})\n",
    "from plotnine import ggplot, aes, geom_histogram, facet_wrap, labs\n",
    "\n",
    "# Adjust the DataFrame slightly for ggplot2-style plotting\n",
    "data['Value'] = data['Value'].astype(float)\n",
    "\n",
    "# Using ggplot from plotnine\n",
    "plot = (ggplot(data, aes(x='Value', fill='Group')) + \n",
    "        geom_histogram(bins=20, position='dodge') + \n",
    "        facet_wrap('~Group') + \n",
    "        labs(title='Subspace Values', x='Value', y='Count'))\n",
    "\n",
    "plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
