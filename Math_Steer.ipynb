{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyvene/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, copy, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "    \n",
    "def extract_answer_number(sentence: str) -> float:\n",
    "    sentence = sentence.replace(',', '')\n",
    "    pred = [s for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n",
    "    if not pred:\n",
    "        return float('inf')\n",
    "    pred_answer = float(pred[-1])\n",
    "    if isinstance(pred_answer, str):\n",
    "        try:\n",
    "            pred_answer = float(pred_answer)\n",
    "        except ValueError as e:\n",
    "            pred_answer = float('inf')\n",
    "    return pred_answer\n",
    "\n",
    "\n",
    "def extract_answer_letter(sentence: str) -> str:\n",
    "    sentence_ = sentence.strip()\n",
    "    pred_answers = re.findall(r'A|B|C|D|E', sentence_)\n",
    "    if pred_answers:\n",
    "        if not pred_answers:\n",
    "            return ''\n",
    "        return pred_answers[-1]\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "device = \"cuda\"\n",
    "prompt_template = \"\"\"Below is an instruction that \\\n",
    "describes a task, paired with an input that provides \\\n",
    "further context. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Input:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "trigger_tokens = \"### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _, llama = create_llama(\"huggyllama/llama-7b\")\n",
    "_ = llama.to(device)  # single gpu\n",
    "_ = llama.eval()  # always no grad on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "tokenizer.padding_side = \"right\" \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "584f16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_dataset = jload(\"./datasets/math_10k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52b50528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'At the arcade Dave won 11 tickets . If he spent 5 tickets on a beanie and later won 10 more tickets , how many would he have ?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Dave initially had 11 tickets. He spent 5 tickets on a beanie, leaving him with:\\n\\n11 - 5 = 6 tickets\\n\\nHe later won 10 more tickets, so his total number of tickets would be:\\n\\n6 + 10 = 16 tickets\\n\\nTherefore, Dave would have 16 tickets after spending 5 tickets on a beanie and winning 10 more tickets. The answer in Arabic numerals is:\\n\\n16',\n",
       "  'answer': '16.0'},\n",
       " {'instruction': 'Anthony has 16 pets. This morning he forgot to lock the door and he lost 6 pets. After that 1/5 of his pets died from old age. How many pets does he have left?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Step 1: Subtract the number of lost pets from the total number of pets to find how many pets Anthony has left: \\n16 - 6 = 10\\n\\nStep 2: Find how many pets died from old age by multiplying the number of remaining pets by 1/5: \\n10 x 1/5 = 2\\n\\nStep 3: Subtract the number of pets that died from old age from the number of remaining pets: \\n10 - 2 = 8\\n\\nTherefore, Anthony has 8 pets left. The answer in Arabic numerals is 8.',\n",
       "  'answer': '8.0'},\n",
       " {'instruction': 'If you take the fastest speed of a rabbit and double it, add 4 to that then double it again you get 188.  How fast is the rabbit?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Let\\'s call the speed of the rabbit \"x\".\\n\\nThe problem tells us to take the fastest speed of a rabbit and double it, so we have:\\n\\n2x\\n\\nThen we add 4 to that:\\n\\n2x + 4\\n\\nAnd finally, we double it again to get 188:\\n\\n2(2x + 4) = 188\\n\\nSimplifying the equation, we can distribute the 2:\\n\\n4x + 8 = 188\\n\\nSubtracting 8 from both sides:\\n\\n4x = 180\\n\\nDividing both sides by 4:\\n\\nx = 45\\n\\nTherefore, the speed of the rabbit is 45.',\n",
       "  'answer': '45.0'},\n",
       " {'instruction': 'Janet counts 30 crows on the powerlines and 60% more hawks than crows. How many birds does she count total?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Step 1: Find 60% of 30 crows.\\n60% of 30 = 0.60 x 30 = 18\\n\\nStep 2: Add the number of hawks to the number of crows.\\nNumber of hawks = 30 crows + 18 additional hawks\\nNumber of hawks = 48\\n\\nStep 3: Add the number of crows and hawks together.\\nTotal number of birds = 30 crows + 48 hawks\\nTotal number of birds = 78\\n\\nTherefore, Janet counts a total of 78 birds.',\n",
       "  'answer': '78.0'},\n",
       " {'instruction': 'Shara collects shells. She had 20 shells before she went on vacation. On vacation, she found 5 shells per day for 3 days. Then he found 6 shells on the fourth day. How many shells does she have now?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Step 1: Find the total number of shells she found on the first 3 days by multiplying the number of shells found per day by the number of days:\\n5 shells/day * 3 days = 15 shells\\n\\nStep 2: Add the number of shells she found on the fourth day:\\n15 shells + 6 shells = 21 shells\\n\\nStep 3: Add the number of shells she had before vacation:\\n20 shells + 21 shells = 41 shells\\n\\nAnswer: Shara has 41 shells now. (41)',\n",
       "  'answer': '41.0'},\n",
       " {'instruction': 'Darnel sprinted 0.875 of a lap and then took a break by jogging 0.75 of a lap. How much farther did Darnel sprint than jog?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Step 1: Find the total distance Darnel covered.\\nTotal distance = distance sprinted + distance jogged\\nTotal distance = 0.875 + 0.75\\nTotal distance = 1.625 laps\\n\\nStep 2: Find the distance Darnel sprinted.\\nDistance sprinted = 0.875 laps\\n\\nStep 3: Find the distance Darnel jogged.\\nDistance jogged = 0.75 laps\\n\\nStep 4: Find the difference between the distance Darnel sprinted and jogged.\\nDifference = Distance sprinted - Distance jogged\\nDifference = 0.875 - 0.75\\nDifference = 0.125 laps\\n\\nTherefore, Darnel sprinted 0.875 laps and jogged 0.75 laps. Darnel sprinted 0.125 laps farther than jogged. The answer in arabic numerals is 0.125.',\n",
       "  'answer': '0.125'},\n",
       " {'instruction': 'Austin bought his seven friends each a robot. Each robot costs $8.75.  He was charged $7.22 total for tax. He left with $11.53 in change. How much did Austin start with?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'Step 1: Find the total cost of the robots.\\n7 friends x $8.75 per robot = $61.25\\n\\nStep 2: Add the tax to the total cost of the robots.\\n$61.25 + $7.22 = $68.47\\n\\nStep 3: Subtract the total cost with tax from the amount Austin had after the purchase to find out how much he started with.\\n$11.53 + $68.47 = $80.00 (amount Austin started with)\\n\\nAnswer: Austin started with $80.00.',\n",
       "  'answer': '80.0'},\n",
       " {'instruction': 'There were 28 bales of hay in the barn . Tim stacked bales in the barn today . There are now 54 bales of hay in the barn . How many bales did he store in the barn ?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'To solve this problem, we need to use subtraction. We need to subtract the initial number of bales from the current number of bales to find out how many bales Tim stored in the barn.\\n\\n54 - 28 = 26\\n\\nTherefore, Tim stored 26 bales in the barn. The answer in Arabic numerals is 26.',\n",
       "  'answer': '26.0'},\n",
       " {'instruction': 'There are 46 children in the classroom , each student will get 4 dozen pencils . How many pencils will the teacher have to give out ?\\n ',\n",
       "  'input': '',\n",
       "  'output': 'First, we need to convert the number of children to the total number of pencils required. \\n\\n46 children x 4 dozen pencils = 184 dozen pencils \\n\\nNext, we need to convert the dozens to individual pencils. \\n\\n184 dozen pencils x 12 individual pencils = 2,208 individual pencils \\n\\nTherefore, the teacher will have to give out 2,208 pencils. \\n\\nArabic numeral answer: 2,208',\n",
       "  'answer': '2208.0'},\n",
       " {'instruction': 'A luxury bag costs $3000. A reseller wants to get a 15% profit. How much should she sell the bag?\\n ',\n",
       "  'input': '',\n",
       "  'output': \"To solve the problem, we have to add the reseller's desired profit to the cost of the bag to get the selling price. Here are the steps:\\n\\n1. Convert the percentage into a decimal: 15% = 0.15\\n2. Multiply the cost of the bag by the decimal equivalent of the reseller's desired profit: 0.15 x $3000 = $450\\n3. Add the profit to the cost of the bag: $3000 + $450 = $3450\\n\\nTherefore, the reseller should sell the bag for $3450.\",\n",
       "  'answer': '3450.0'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "for data_item in math_dataset:\n",
    "    base_prompt = prompt_template % (data_item['instruction'], data_item['input'])\n",
    "    # base input = base prompt + steered base output\n",
    "    base_input = base_prompt + data_item[\"output\"] + tokenizer.pad_token\n",
    "    base_prompt_length = len(tokenizer(\n",
    "        base_prompt, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    base_input_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = tokenizer(\n",
    "        base_input, max_length=512, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids[:base_prompt_length] = -100\n",
    "    \n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=llama,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "initial_lr = 5e-3\n",
    "total_step = 0\n",
    "gradient_accumulation_steps = 2\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "    \n",
    "class ConditionedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(torch.bfloat16)\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source(base) - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "    \n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": 2,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 8},{\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 8},{\n",
    "    \"layer\": 18,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 8},{\n",
    "    \"layer\": 26,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 8}],\n",
    "    ConditionedSourceLowRankRotatedSpaceIntervention\n",
    ")\n",
    "intervenable = IntervenableModel(config, llama)\n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, end_factor=0.1, total_iters=epochs\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        base_first_token = torch.zeros_like(inputs[\"intervention_position\"]).tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            unit_locations={\"sources->base\": (None, [\n",
    "                base_unit_location\n",
    "            ]*4)})\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        lm_logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tasks = [\n",
    "    \"MultiArith\", \"gsm8k\", \"AddSub\", \"AQuA\", \"SingleEq\", \"SVAMP\"\n",
    "]\n",
    "for i in range(len(eval_tasks)):\n",
    "    eval_dataset = jload(f\"./datasets/{eval_tasks[i]}/test.json\")\n",
    "    max_sample_examples = min(1, len(eval_dataset)) # hellaswag is larger, let's do full eval later!\n",
    "    sampled_items = random.sample(range(len(eval_dataset)), max_sample_examples)\n",
    "    sampled_eval_dataset = [eval_dataset[idx] for idx in sampled_items]\n",
    "    correct_count = 0\n",
    "    totol_count = 0\n",
    "    eval_iterator = tqdm(\n",
    "        sampled_eval_dataset, position=0, leave=True\n",
    "    )\n",
    "    for data_item in eval_iterator:\n",
    "        prompt = prompt_template % (data_item['instruction'], data_item['input'])\n",
    "        # base input = base prompt + steered base output\n",
    "        answer = data_item[\"answer\"]\n",
    "        prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        base_unit_location = prompt[\"input_ids\"].shape[-1] - 1 \n",
    "        _, steered_response = intervenable.generate(\n",
    "            prompt, \n",
    "            unit_locations={\"base\": base_unit_location},\n",
    "            intervene_on_prompt=True,\n",
    "            max_new_tokens=512, do_sample=False, \n",
    "            eos_token_id=tokenizer.pad_token_id, early_stopping=True\n",
    "        )\n",
    "        raw_generation = tokenizer.decode(steered_response[0], skip_special_tokens=True)\n",
    "        generation = raw_generation.split(trigger_tokens)[1]\n",
    "        \n",
    "        generation = generation.strip()\n",
    "        if eval_tasks[i] == \"AQuA\":\n",
    "            generation = extract_answer_letter(generation)\n",
    "            if generation.strip() == answer.strip():\n",
    "                correct_count += 1\n",
    "        else:\n",
    "            generation = extract_answer_number(generation)\n",
    "            if generation == float(answer):\n",
    "                correct_count += 1\n",
    "        totol_count += 1\n",
    "        \n",
    "        print(data_item['instruction'], raw_generation, generation, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113b281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
