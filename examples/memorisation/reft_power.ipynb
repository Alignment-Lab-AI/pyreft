{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafcbe5b-b1bb-42c5-930c-98129462e989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e157b6b646e64b8fb10414068d098097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import ggplot, aes, geom_line, theme_minimal\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    ReftSupervisedDataset\n",
    ")\n",
    "# for customized interventions\n",
    "from pyvene import (\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, model, storage_access_ids, memo_sequences) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    all_base_input_ids, all_intervention_locations, all_output_ids = [], [], []\n",
    "    for i in range(len(storage_access_ids)):\n",
    "        storage_access_id = storage_access_ids[i]\n",
    "        memo_sequence = memo_sequences[i]\n",
    "    \n",
    "        base_prompt = storage_access_id\n",
    "        base_input = base_prompt + memo_sequence + tokenizer.eos_token\n",
    "    \n",
    "        # tokenize\n",
    "        base_prompt_ids = tokenizer(\n",
    "            base_prompt, max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        base_prompt_length = len(base_prompt_ids)\n",
    "        base_input_ids = tokenizer(\n",
    "            base_input, max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        output_ids = copy.deepcopy(base_input_ids)\n",
    "        output_ids[:base_prompt_length] = IGNORE_INDEX\n",
    "        \n",
    "        all_base_input_ids.append(base_input_ids)\n",
    "        all_intervention_locations.append([[base_prompt_length - 1]])\n",
    "        all_output_ids.append(output_ids)\n",
    "        \n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": all_base_input_ids,\n",
    "        \"intervention_locations\": all_intervention_locations,\n",
    "        \"labels\": all_output_ids,\n",
    "    })\n",
    "        \n",
    "    data_collator_fn = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    data_collator = ReftDataCollator(data_collator=data_collator_fn)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "\n",
    "# load model (take 1 min)\n",
    "model_name_or_path = \"yahma/llama-7b-hf\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b57de6-b25d-451a-aa3e-0cd75e8d5938",
   "metadata": {},
   "source": [
    "### 1-D linear subspace in LMs is a disk storage unit\n",
    "\n",
    "We try to store a random short sequence in a 1-D linear subspace of the last prompt token.\n",
    "```py\n",
    "memo_sequence=\"Hey! This is Zhengxuan working on random stuff with LLaMA models!\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47369b7-a22b-4fd8-be7d-fee29395a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 4,097 || trainable model params: 0\n",
      "model params: 6,738,415,616 || trainable%: 6.080064266549391e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:23, Epoch 500/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memo_sequence=\"Hey! This is Zhengxuan working on random stuff with LLaMA models!\"\n",
    "TARGET_LAYER = 15\n",
    "\n",
    "storage_access_id = \"RAND#ID1->\"\n",
    "model_max_length = 2048\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1)})\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "reft_model.print_trainable_parameters()\n",
    "\n",
    "# get training data and args\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, model=model, storage_access_ids=[storage_access_id], memo_sequences=[memo_sequence])\n",
    "training_args = transformers.TrainingArguments(output_dir=\"./tmp\")\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"no\"\n",
    "training_args.num_train_epochs = 500.0\n",
    "training_args.learning_rate = 2e-2\n",
    "training_args.per_device_train_batch_size = 1\n",
    "training_args.report_to = []\n",
    "training_args.logging_steps = 100\n",
    "\n",
    "# train\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f2a5f-9c0b-4622-97f2-7c2696cd7d46",
   "metadata": {},
   "source": [
    "### Check stored data\n",
    "\n",
    "We can ask our `reft_model` to generate something with different prompt prefix to make sure the output sequence is now stored in the linear subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5213fbc-3cdd-4376-8995-8aa3159700e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAND#ID1->Hey! This is Zhengxuan working on random stuff with LLaMA models!\n"
     ]
    }
   ],
   "source": [
    "storage_access_id = \"RAND#ID1->\"\n",
    "\n",
    "prompt = tokenizer(storage_access_id, return_tensors=\"pt\").to(device)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1\n",
    "_, steered_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=1024, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a845d4-59c0-4683-bee2-1bb9d5db6cdb",
   "metadata": {},
   "source": [
    "How large is the `4097` vector file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c137f610-a6f4-43e2-9b69-bad95ce2e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key = list(reft_model.interventions.keys())[0]\n",
    "weight_storage_dict = {\n",
    "    \"1d_intervention_w\": reft_model.interventions[_key][0].rotate_layer.weight.detach().data,\n",
    "    \"1d_intervention_scalar\": reft_model.interventions[_key][0].learned_source.detach().data\n",
    "}\n",
    "torch.save(weight_storage_dict, './tmp/1d_storage.pt') # 17.5 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb67b4-7fdb-4922-8afe-8618cb1ab896",
   "metadata": {},
   "source": [
    "The saved file is 17.5 KB. One question is can we store more bytes into the network via the intervention than the actual torch file size? Given LLMs are pretrained with billions of tokens, it seems possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531c72f-d171-41e1-bee8-a9ee84ccf1d5",
   "metadata": {},
   "source": [
    "### Store with different access id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6122a4-6da8-4d18-aa8c-f7ee1667b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_f = open('./alice_in_wonderland.txt', 'r')\n",
    "alice_content = alice_f.readlines()\n",
    "alice_book = \"\\n\".join(alice_content)\n",
    "\n",
    "num_char = 2000 # about the same as number of bytes, 2000 chars ~= 2KB\n",
    "alice_slice = alice_book[:num_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6df2450a-6e48-41bf-a749-d535f5543f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 4,097 || trainable model params: 0\n",
      "model params: 6,738,415,616 || trainable%: 6.080064266549391e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:46, Epoch 500/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET_LAYER = 15\n",
    "\n",
    "alice_access_id = \"ALIC#ID1->\"\n",
    "model_max_length = 2048\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1)})\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "reft_model.print_trainable_parameters()\n",
    "\n",
    "# get training data and args\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, model=model, \n",
    "    storage_access_ids=[storage_access_id, alice_access_id], memo_sequences=[memo_sequence, alice_slice])\n",
    "training_args = transformers.TrainingArguments(output_dir=\"./tmp\")\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"no\"\n",
    "training_args.num_train_epochs = 500.0\n",
    "training_args.learning_rate = 8e-3\n",
    "training_args.per_device_train_batch_size = 2\n",
    "training_args.report_to = []\n",
    "training_args.logging_steps = 100\n",
    "\n",
    "# train\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "829fd7b3-49e1-456a-8c3d-6b7d69192d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAND#ID1->Hey! This is Zhengxuan working on random stuff with LLaMA models!\n"
     ]
    }
   ],
   "source": [
    "storage_access_id = \"RAND#ID1->\"\n",
    "\n",
    "prompt = tokenizer(storage_access_id, return_tensors=\"pt\").to(device)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1\n",
    "_, steered_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=1024, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bee955d4-9570-41dd-aae6-e91a2ed862b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored token num: 585\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "storage_access_id = \"ALIC#ID1->\"\n",
    "\n",
    "prompt = tokenizer(storage_access_id, return_tensors=\"pt\").to(device)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1\n",
    "_, steered_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=2048, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "retrieved_storage = tokenizer.decode(steered_response[0], skip_special_tokens=True)\n",
    "print(\"stored token num:\", len(data_module['train_dataset'][1]['input_ids']))\n",
    "print(retrieved_storage.split(\"ALIC#ID1->\")[-1]==alice_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0911b-b3d4-4ec3-8860-f8321e85968f",
   "metadata": {},
   "source": [
    "### Storage at different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39882671-1e21-42b9-926f-fc417c63bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 4,097 || trainable model params: 0\n",
      "model params: 6,738,415,616 || trainable%: 6.080064266549391e-05\n",
      "stored token num: 4096\n"
     ]
    }
   ],
   "source": [
    "num_char = 18000 # about the same as number of bytes, 2000 chars ~= 2KB\n",
    "alice_slice = alice_book[:num_char]\n",
    "\n",
    "TARGET_LAYER = 15\n",
    "\n",
    "alice_access_id = \"ALIC#ID1->\"\n",
    "model_max_length = 4096\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1)})\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "reft_model.print_trainable_parameters()\n",
    "\n",
    "# get training data and args\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, model=model, \n",
    "    storage_access_ids=[alice_access_id], memo_sequences=[alice_slice])\n",
    "print(\"stored token num:\", len(data_module['train_dataset'][0]['input_ids']))\n",
    "training_args = transformers.TrainingArguments(output_dir=\"./tmp\")\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"no\"\n",
    "training_args.num_train_epochs = 500.0\n",
    "training_args.learning_rate = 2e-3\n",
    "training_args.per_device_train_batch_size = 1\n",
    "training_args.report_to = []\n",
    "training_args.logging_steps = 100\n",
    "\n",
    "# train\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba2d72-9519-484f-9ece-577b15128bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 67/500 00:53 < 05:55, 1.22 it/s, Epoch 66/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d3c370a2-19f8-47b6-8d52-fd821958244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n"
     ]
    }
   ],
   "source": [
    "storage_access_id = \"ALIC#ID1->\"\n",
    "\n",
    "prompt = tokenizer(storage_access_id, return_tensors=\"pt\").to(device)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1\n",
    "_, steered_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=2500, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "retrieved_storage = tokenizer.decode(steered_response[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c401a213-5a70-4c01-9906-a968f6663b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_char_level_accuracy(retrieved, golden):\n",
    "    n_c, n = 0, 0\n",
    "    accum_acc = []\n",
    "    for char in retrieved:\n",
    "        if char == golden[n]:\n",
    "            n_c += 1\n",
    "        n += 1\n",
    "        accum_acc += [round(n_c/n, 2)]\n",
    "    \n",
    "    # Define the DataFrame again in case it's needed\n",
    "    df = pd.DataFrame({\n",
    "        'Index': range(1, len(accum_acc) + 1),\n",
    "        'Values': accum_acc\n",
    "    })\n",
    "    \n",
    "    # Create the line plot\n",
    "    plot = (\n",
    "        ggplot(df, aes(x='Index', y='Values')) +\n",
    "        geom_line() +  # Draw the line\n",
    "        theme_minimal()  # Use a minimalistic theme\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac26db5e-6358-4635-8c66-31afb4226b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plot_char_level_accuracy(retrieved_storage.split(\"ALIC#ID1->\")[-1], alice_slice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
