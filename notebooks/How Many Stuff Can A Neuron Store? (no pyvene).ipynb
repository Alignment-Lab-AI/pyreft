{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96158ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_word import RandomWords\n",
    "r = RandomWords()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../pyvene/\")\n",
    "\n",
    "import torch\n",
    "import random, copy, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.activations import ACT2FN\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "prompt_template = \"\"\"Below is an instruction that \\\n",
    "describes a task. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "trigger_tokens = \"### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce68e36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2ea60e6214847a15c18c52560494a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at huggyllama/llama-7b and are newly initialized: ['model.strange_patcher.learned_source.bias', 'model.strange_patcher.learned_source.weight', 'model.strange_patcher.proj_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new tokens count:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "from models.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "llama = LlamaForCausalLM.from_pretrained(\n",
    "    \"huggyllama/llama-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "_ = llama.to(device) # single gpu\n",
    "_ = llama.eval()  # always no grad on the model\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "tokenizer.padding_side = \"right\" \n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "llama.resize_token_embeddings(len(tokenizer))\n",
    "print(\"adding new tokens count: \", num_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9906fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0160, -0.0267, -0.0130,  ..., -0.0037,  0.0286,  0.0023]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0028,  0.0183,  0.0211,  ..., -0.0129, -0.0039, -0.0055]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True), Parameter containing:\n",
      "tensor([0.], device='cuda:0', dtype=torch.bfloat16, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers\n",
    "for param in llama.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze layers based on a naming convention, e.g., names containing \"strange_patcher\"\n",
    "for name, param in llama.named_parameters():\n",
    "    if \"strange_patcher\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "params_to_optimize = [param for param in llama.parameters() if param.requires_grad]\n",
    "print(params_to_optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393704d0",
   "metadata": {},
   "source": [
    "We construct a strange dataset where given a random, the model needs to generate a random English sentence in return.\n",
    "\n",
    "Then, we train to train a 1-D linear subspace intervention that could steer the model to generate the random English sentence.\n",
    "\n",
    "We vary the random English sentence length to measure how much that 1-D linear subspace could store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91759ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing site\n",
    "rerun = 3\n",
    "epochs = 500\n",
    "initial_lr = 5e-3\n",
    "        \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rerun_idx in range(rerun):\n",
    "    for random_len in [32, 64, 128, 256, 512, 1024]:\n",
    "        print(f\"{rerun_idx} and {random_len}\")\n",
    "        all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "        prompt = \"This is a random prompt.\"\n",
    "        base_prompt = prompt_template % prompt\n",
    "        overwrite_output = [r.get_random_word() for _ in range(random_len)]\n",
    "        base_input = base_prompt + \" \".join(overwrite_output) + tokenizer.pad_token\n",
    "\n",
    "        base_prompt_length = len(tokenizer(\n",
    "            # we use 256 to follow previous work's cut-off length\n",
    "            base_prompt, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "        base_input_ids = tokenizer(\n",
    "            base_input, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        output_ids = tokenizer(\n",
    "            base_input, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        base_input_ids[-1] = tokenizer.pad_token_id\n",
    "        output_ids[-1] = tokenizer.pad_token_id\n",
    "        output_ids[:base_prompt_length] = -100\n",
    "\n",
    "        all_base_input_ids.append(base_input_ids)\n",
    "        all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "        all_output_ids.append(output_ids)\n",
    "\n",
    "        raw_train = (\n",
    "            all_base_input_ids,\n",
    "            all_base_positions,\n",
    "            all_output_ids,\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_dict(\n",
    "            {\n",
    "                \"input_ids\": raw_train[0],\n",
    "                \"intervention_position\": raw_train[1],\n",
    "                \"labels\": raw_train[2],\n",
    "            }\n",
    "        ).shuffle(seed=42)\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer,\n",
    "            model=llama,\n",
    "            label_pad_token_id=-100,\n",
    "            padding=\"longest\"\n",
    "        )\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "        total_step = 0\n",
    "        gradient_accumulation_steps = 1\n",
    "        batch_size = 1\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params_to_optimize, lr=initial_lr)\n",
    "\n",
    "        for epoch in range(0, int(epochs)):\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                for k, v in inputs.items():\n",
    "                    if v is not None and isinstance(v, torch.Tensor):\n",
    "                        inputs[k] = v.to(device)\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "                base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "                base_first_token = torch.zeros_like(inputs[\"intervention_position\"]).tolist()\n",
    "                cf_outputs = llama(**{\"input_ids\": inputs[\"input_ids\"]})\n",
    "\n",
    "                # lm loss on counterfactual labels\n",
    "                lm_logits = cf_outputs.logits\n",
    "                labels = inputs[\"labels\"]\n",
    "                shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss_str = round(loss.item(), 2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                total_step += 1\n",
    "                if loss_str == 0.0:\n",
    "                    break\n",
    "        if random_len not in results:\n",
    "            results[random_len] = [loss_str]\n",
    "        else:\n",
    "            results[random_len] += [loss_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
