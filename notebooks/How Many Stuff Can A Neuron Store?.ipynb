{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8462d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_word import RandomWords\n",
    "r = RandomWords()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../pyvene/\")\n",
    "\n",
    "import torch\n",
    "import random, copy, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.activations import ACT2FN\n",
    "import wandb\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama, create_gpt2_lm\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "prompt_template = \"\"\"Below is an instruction that \\\n",
    "describes a task. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "trigger_tokens = \"### Response:\\n\"\n",
    "\n",
    "class SourceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(1, self.embed_dim), requires_grad=True)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        output = self.learned_source\n",
    "        return output.to(base.dtype)\n",
    "\n",
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return output.to(base.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _, llama = create_gpt2_lm(\"gpt2-xl\")\n",
    "_ = llama.to(device)  # single gpu\n",
    "_ = llama.eval()  # always no grad on the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer.padding_side = \"right\" \n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "llama.resize_token_embeddings(len(tokenizer))\n",
    "print(\"adding new tokens count: \", num_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dddeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing site\n",
    "rank = 1\n",
    "layers = \"32\"\n",
    "rerun = 3\n",
    "epochs = 500\n",
    "initial_lr = 8e-3\n",
    "        \n",
    "results = {}\n",
    "\n",
    "layers = [int(l) for l in layers.split(\";\")]\n",
    "config = IntervenableConfig([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank} for l in layers],\n",
    "    LearnedSourceLowRankRotatedSpaceIntervention\n",
    ")\n",
    "llama.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rerun_idx in range(rerun):\n",
    "    for random_len in [32, 64, 128, 256, 512, 1024]:\n",
    "        print(f\"{rerun_idx} and {random_len}\")\n",
    "        all_base_input_ids, all_base_positions, all_output_ids, all_source_input_ids = [], [], [], []\n",
    "\n",
    "        prompt = \"This is a random prompt.\"\n",
    "        base_prompt = prompt_template % prompt\n",
    "        overwrite_output = [r.get_random_word() for _ in range(random_len)]\n",
    "        base_input = base_prompt + \" \".join(overwrite_output) + tokenizer.pad_token\n",
    "\n",
    "        base_prompt_length = len(tokenizer(\n",
    "            # we use 256 to follow previous work's cut-off length\n",
    "            base_prompt, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "        base_input_ids = tokenizer(\n",
    "            base_input, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        output_ids = tokenizer(\n",
    "            base_input, max_length=2048, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        base_input_ids[-1] = tokenizer.pad_token_id\n",
    "        output_ids[-1] = tokenizer.pad_token_id\n",
    "        output_ids[:base_prompt_length] = -100\n",
    "        \n",
    "        all_base_input_ids.append(base_input_ids)\n",
    "        all_base_positions.append([base_prompt_length-1]) # intervene on the last prompt token\n",
    "        all_output_ids.append(output_ids)\n",
    "\n",
    "        raw_train = (\n",
    "            all_base_input_ids,\n",
    "            all_base_positions,\n",
    "            all_output_ids,\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_dict(\n",
    "            {\n",
    "                \"input_ids\": raw_train[0],\n",
    "                \"intervention_position\": raw_train[1],\n",
    "                \"labels\": raw_train[2],\n",
    "            }\n",
    "        ).shuffle(seed=42)\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer,\n",
    "            model=llama,\n",
    "            label_pad_token_id=-100,\n",
    "            padding=\"longest\"\n",
    "        )\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "        total_step = 0\n",
    "        gradient_accumulation_steps = 1\n",
    "        batch_size = 1\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "        intervenable = IntervenableModel(config, llama)\n",
    "        intervenable.set_device(device)\n",
    "        intervenable.disable_model_gradients()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(\n",
    "            intervenable.get_trainable_parameters(), lr=initial_lr\n",
    "        )\n",
    "        intervenable.model.train()  # train enables drop-off but no grads\n",
    "\n",
    "        for epoch in range(0, int(epochs)):\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                for k, v in inputs.items():\n",
    "                    if v is not None and isinstance(v, torch.Tensor):\n",
    "                        inputs[k] = v.to(device)\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "                base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "                base_first_token = torch.zeros_like(inputs[\"intervention_position\"]).tolist()\n",
    "                _, cf_outputs = intervenable(\n",
    "                    {\"input_ids\": inputs[\"input_ids\"]},\n",
    "                    unit_locations={\"sources->base\": (None,[\n",
    "                        base_unit_location\n",
    "                    ])})\n",
    "\n",
    "                # lm loss on counterfactual labels\n",
    "                lm_logits = cf_outputs.logits\n",
    "                labels = inputs[\"labels\"]\n",
    "                shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss_str = round(loss.item(), 2)\n",
    "                loss.backward()\n",
    "                print(loss_str)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                total_step += 1\n",
    "                if loss_str == 0.0:\n",
    "                    break\n",
    "        if random_len not in results:\n",
    "            results[random_len] = [loss_str]\n",
    "        else:\n",
    "            results[random_len] += [loss_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c9f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
