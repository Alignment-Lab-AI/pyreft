{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "056a1909-8ab9-4e5c-b0ed-73f9e2a81933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyvene/\")\n",
    "\n",
    "import torch\n",
    "import random, copy, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers import default_data_collator\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from pyvene import create_llama\n",
    "from pyvene import set_seed, count_parameters\n",
    "from pyvene.models.layers import LowRankRotateLayer\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "        \n",
    "device = \"cuda\"\n",
    "\n",
    "class LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Parameter(\n",
    "            torch.rand(kwargs[\"low_rank_dimension\"]), requires_grad=True)\n",
    "        self.dropout = torch.nn.Dropout(0.05)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.learned_source - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return self.dropout(output.to(base.dtype))\n",
    "\n",
    "class ConditionedSourceLowRankRotatedSpaceIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"])\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        self.learned_source = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(torch.bfloat16)\n",
    "        self.act_fn = ACT2FN[\"tanh\"]\n",
    "        self.dropout = torch.nn.Dropout(0.05)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return self.dropout(output.to(base.dtype))\n",
    "    \n",
    "class ConditionedSourceLowRankIntervention(\n",
    "    ConstantSourceIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.proj_layer = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"], bias=False).to(torch.bfloat16)\n",
    "        self.learned_source = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(torch.bfloat16)\n",
    "        self.act_fn = ACT2FN[\"tanh\"]\n",
    "        self.dropout = torch.nn.Dropout(0.05)\n",
    "        \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        proj_base = self.proj_layer(base)\n",
    "        output = base + torch.matmul(\n",
    "            (self.act_fn(self.learned_source(base)) - proj_base), self.proj_layer.weight\n",
    "        )\n",
    "        return self.dropout(output.to(base.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea5f2e0f-b39c-4c18-b5bd-8977c6ccc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\",\n",
    "    num_labels=2,\n",
    "    finetuning_task=\"sst2\",\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\",\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1763287-3afb-4573-82d9-707361ac2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "train_datasets = raw_datasets[\"train\"]\n",
    "all_base_input_ids, all_base_positions, all_output_ids = [], [], []\n",
    "for data_item in train_datasets:\n",
    "    base_input_ids = tokenizer(\n",
    "        data_item[\"sentence\"], max_length=512, \n",
    "        truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = data_item[\"label\"]\n",
    "\n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([0]) # intervene on the first prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_train = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"intervention_position\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "    }\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "train_dataset\n",
    "num_labels = 2\n",
    "\n",
    "test_datasets = raw_datasets[\"validation\"]\n",
    "all_base_input_ids, all_base_positions, all_output_ids = [], [], []\n",
    "for data_item in test_datasets:\n",
    "    base_input_ids = tokenizer(\n",
    "        data_item[\"sentence\"], max_length=512, \n",
    "        truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    output_ids = data_item[\"label\"]\n",
    "\n",
    "    all_base_input_ids.append(base_input_ids)\n",
    "    all_base_positions.append([0]) # intervene on the first prompt token\n",
    "    all_output_ids.append(output_ids)\n",
    "\n",
    "raw_test = (\n",
    "    all_base_input_ids,\n",
    "    all_base_positions,\n",
    "    all_output_ids,\n",
    ")\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_test[0],\n",
    "        \"intervention_position\": raw_test[1],\n",
    "        \"labels\": raw_test[2],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fdf9bf2-76ea-4b9c-9f65-6f172bd67ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta trainable parameters:  0\n",
      "intervention trainable parameters:  6148\n"
     ]
    }
   ],
   "source": [
    "layers = [5, 11] # 5, 11\n",
    "rank = 2\n",
    "initial_lr = 6e-3\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "config = IntervenableConfig([{\n",
    "    \"component\": f\"roberta.encoder.layer[{l}].output\",\n",
    "    \"intervention\": ConditionedSourceLowRankRotatedSpaceIntervention(\n",
    "        embed_dim=model.config.hidden_size, low_rank_dimension=rank)\n",
    "} for l in layers],\n",
    "    \n",
    ")\n",
    "intervenable = IntervenableModel(config, model)\n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    intervenable.get_trainable_parameters(), lr=initial_lr,\n",
    "    weight_decay=False\n",
    ")\n",
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"roberta trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "\n",
    "total_step = 0\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "t_total = int(len(train_dataloader) * epochs) // gradient_accumulation_steps\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(t_total*0.1), \n",
    "    num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2e77209-e0aa-43b2-b593-63d55f6fc7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robertaformaskedlm'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.architectures[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34723f5a-a160-4d81-8b93-8832898d140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2105/2105 [00:42<00:00, 49.47it/s, loss=0.4, acc=0.9]\n",
      "Epoch:  33%|███████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                      | 1/3 [00:42<01:25, 42.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.921     0.869     0.894       428\n",
      "           1      0.880     0.928     0.904       444\n",
      "\n",
      "    accuracy                          0.899       872\n",
      "   macro avg      0.901     0.899     0.899       872\n",
      "weighted avg      0.900     0.899     0.899       872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                 | 1636/2105 [00:33<00:09, 49.64it/s, loss=0.2, acc=0.9]"
     ]
    }
   ],
   "source": [
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "\n",
    "    intervenable.model.train()  # train enables drop-off but no grads\n",
    "    for k,v in intervenable.interventions.items():\n",
    "        _ = v[0].train()\n",
    "        \n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "            unit_locations={\"sources->base\": (None,[base_unit_location]*len(layers))})\n",
    "\n",
    "        # lm loss on counterfactual labels\n",
    "        logits = cf_outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        loss_str = round(loss.item(), 1)\n",
    "\n",
    "        acc = round(((logits.argmax(dim=-1) == labels).sum()/b_s).tolist(), 1)\n",
    "        \n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str, \"acc\": acc})\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        total_step += 1\n",
    "\n",
    "    # ensure everything is in eval mode\n",
    "    intervenable.model.eval()\n",
    "    for k,v in intervenable.interventions.items():\n",
    "        _ = v[0].eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for inputs in test_dataloader:\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "    \n",
    "        base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "            unit_locations={\"sources->base\": (None,[base_unit_location]*len(layers))})\n",
    "    \n",
    "        # lm loss on counterfactual labels\n",
    "        preds = cf_outputs.logits.argmax(dim=-1)\n",
    "        labels = inputs[\"labels\"]\n",
    "        all_preds += preds.tolist()\n",
    "        all_labels += labels.tolist()\n",
    "    print(classification_report(all_labels, all_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8235b263-591c-43b3-a4ae-967ff3e4b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.927     0.895     0.911       428\n",
      "           1      0.902     0.932     0.917       444\n",
      "\n",
      "    accuracy                          0.914       872\n",
      "   macro avg      0.915     0.914     0.914       872\n",
      "weighted avg      0.914     0.914     0.914       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ensure everything is in eval mode\n",
    "intervenable.model.eval()\n",
    "for k,v in intervenable.interventions.items():\n",
    "    _ = v[0].eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for inputs in test_dataloader:\n",
    "    for k, v in inputs.items():\n",
    "        if v is not None and isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(device)\n",
    "    b_s = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "    base_unit_location = inputs[\"intervention_position\"].tolist()\n",
    "    _, cf_outputs = intervenable(\n",
    "        {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
    "        unit_locations={\"sources->base\": (None,[base_unit_location]*len(layers))})\n",
    "\n",
    "    # lm loss on counterfactual labels\n",
    "    preds = cf_outputs.logits.argmax(dim=-1)\n",
    "    labels = inputs[\"labels\"]\n",
    "    all_preds += preds.tolist()\n",
    "    all_labels += labels.tolist()\n",
    "print(classification_report(all_labels, all_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e1187-2483-47f5-9a64-af9ab4148851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
